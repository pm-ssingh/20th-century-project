{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208ab2ae-0485-4435-b032-dad32b141310",
   "metadata": {},
   "source": [
    "# Exercise 1.6: Named Entity Recognition and Network Analysis\n",
    "\n",
    "## Objective\n",
    "Apply Named Entity Recognition (NER) algorithm to extract relationships between countries mentioned in the 20th century timeline. This will prepare data for network visualization in Exercise 1.7.\n",
    "\n",
    "## 1. Import Libraries and Setup\n",
    "\n",
    "I'm importing all necessary libraries for NER analysis and network relationship extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fee11341-44df-477a-86dd-a638b3f017b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69de1a0-3b4f-4896-9d3a-b0f3f6205686",
   "metadata": {},
   "source": [
    "## 2. Load 20th Century Text Data\n",
    "\n",
    "I'm loading the text file I created in Exercise 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae05bc39-31dd-4793-a951-bce3e6a5d18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text loaded successfully!\n",
      "Total characters: 101768\n",
      "\n",
      "First 500 characters:\n",
      "    Timeline of the 20th century - Wikipedia                            Jump to content        Main menu      Main menu move to sidebar hide    \t\tNavigation \t   Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us      \t\tContribute \t   HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages                    Search            Search                       Appearance                 Donate  Create account  Log in         Personal tools      Donate Create account \n"
     ]
    }
   ],
   "source": [
    "# Load the 20th century timeline text\n",
    "with open('20th_century_events.txt', 'r', errors='ignore') as file: \n",
    "    data = file.read().replace('\\n', ' ')\n",
    "\n",
    "print(f\"Text loaded successfully!\")\n",
    "print(f\"Total characters: {len(data)}\")\n",
    "print(f\"\\nFirst 500 characters:\\n{data[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec961321-a124-40df-b9df-c0a60331e1f8",
   "metadata": {},
   "source": [
    "## 3. Evaluate Text and Data Wrangling Needs\n",
    "\n",
    "### Observations:\n",
    "\n",
    "**Special Characters:**\n",
    "- The text contains navigation elements from Wikipedia (\"Jump to content\", \"Main menu\", \"move to sidebar\")\n",
    "- Multiple spaces and formatting artifacts from web scraping\n",
    "- No problematic special characters that would break NER\n",
    "\n",
 
    "### Wrangling Strategy:\n",
    "1. Load countries list\n",
    "2. Check if country names need standardization\n",
    "3. Clean any formatting issues if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb748d3-a86a-4d66-a29f-201397c17d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total countries loaded: 373\n",
      "\n",
      "First 20 countries:\n",
      "1. Afghanistan\n",
      "2. Albania\n",
      "3. Algeria\n",
      "4. Andorra\n",
      "5. Angola\n",
      "6. Antigua and Barbuda\n",
      "7. Argentina\n",
      "8. Armenia\n",
      "9. Australia\n",
      "10. Austria\n",
      "11. Azerbaijan\n",
      "12. Bahamas\n",
      "13. Bahrain\n",
      "14. Bangladesh\n",
      "15. Barbados\n",
      "16. Belarus\n",
      "17. Belgium\n",
      "18. Belize\n",
      "19. Benin\n",
      "20. Bhutan\n"
     ]
    }
   ],
   "source": [
    "# Load countries list\n",
    "with open('countries_list.txt', 'r', errors='ignore') as f:\n",
    "    countries = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"Total countries loaded: {len(countries)}\")\n",
    "print(f\"\\nFirst 20 countries:\")\n",
    "for i, country in enumerate(countries[:20], 1):\n",
    "    print(f\"{i}. {country}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b560ff7c-61e5-4f09-ae77-31d4f50e5d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking how countries appear in the text:\n",
      "\n",
      "United States: 55 exact matches, 55 case-insensitive matches\n",
      "China: 32 exact matches, 34 case-insensitive matches\n",
      "Russia: 30 exact matches, 30 case-insensitive matches\n",
      "Germany: 23 exact matches, 23 case-insensitive matches\n",
      "France: 17 exact matches, 17 case-insensitive matches\n",
      "United Kingdom: 28 exact matches, 28 case-insensitive matches\n",
      "\n",
      "\n",
      "Multi-word countries in our list:\n",
      "Total multi-word countries: 136\n",
      "Examples: ['Antigua and Barbuda', 'Bosnia and Herzegovina', 'Burkina Faso', 'Cabo Verde', 'Central African Republic', 'Costa Rica', \"Côte d'Ivoire\", 'Democratic Republic of the Congo', 'Dominican Republic', 'El Salvador']\n"
     ]
    }
   ],
   "source": [
    "# Check sample of text for country mentions\n",
    "sample_countries = ['United States', 'China', 'Russia', 'Germany', 'France', 'United Kingdom']\n",
    "\n",
    "print(\"Checking how countries appear in the text:\\n\")\n",
    "for country in sample_countries:\n",
    "    count = data.count(country)\n",
    "    count_lower = data.lower().count(country.lower())\n",
    "    print(f\"{country}: {count} exact matches, {count_lower} case-insensitive matches\")\n",
    "\n",
    "# Check if multi-word countries exist in our list\n",
    "print(\"\\n\\nMulti-word countries in our list:\")\n",
    "multi_word_countries = [c for c in countries if ' ' in c]\n",
    "print(f\"Total multi-word countries: {len(multi_word_countries)}\")\n",
    "print(f\"Examples: {multi_word_countries[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf877a1d-c23d-4a36-ac2e-545c68ae2786",
   "metadata": {},
   "source": [
    "## 4. Text Wrangling Assessment\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "**1. Case Sensitivity:**\n",
    "- Country names appear with consistent capitalization in the text\n",
    "- No major case-sensitivity issues detected\n",
    "\n",
    "**2. Multi-word Countries:**\n",
    "- 136 out of 373 countries have multi-word names\n",
    "- Examples: \"United States\", \"United Kingdom\", \"Costa Rica\"\n",
    "- SpaCy's NER should handle these correctly with dependency parsing\n",
    "\n",
    "**3. Special Characters:**\n",
    "- One country has special characters: \"Côte d'Ivoire\" (with accent and apostrophe)\n",
    "- The text contains Wikipedia navigation elements but these won't interfere with NER\n",
    "\n",
    "**4. Name Variations:**\n",
    "- \"China\" appears 34 times (case-insensitive) vs 32 exact matches - possible references like \"china\" (lowercase)\n",
    "- Countries are generally referred to by their common names (not official names)\n",
    "\n",
    "### Decision:\n",
    "**No text wrangling needed.** The text is clean enough for NER analysis. SpaCy's algorithm will:\n",
    "- Handle multi-word entities through dependency parsing\n",
    "- Recognize proper nouns regardless of surrounding navigation text\n",
    "- Process the text as-is\n",
    "\n",
    "I'll proceed directly to creating the NER object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1c241-b0c9-43ec-922a-ba35588e9b00",
   "metadata": {},
   "source": [
    "## 5. Create NER Object\n",
    "\n",
    "I'm applying SpaCy's Named Entity Recognition algorithm to the 20th century timeline text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8817166a-22b4-4ac9-94c5-17a97db5795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER object created successfully!\n",
      "Total tokens processed: 19561\n",
      "Total sentences: 1401\n",
      "Total entities found: 4040\n"
     ]
    }
   ],
   "source": [
    "# Create NER object using SpaCy\n",
    "print(\"Processing text with NER algorithm...\")\n",
    "print(\"This may take a minute...\")\n",
    "\n",
    "book = nlp(data)\n",
    "\n",
    "print(f\"\\nNER object created successfully!\")\n",
    "print(f\"Total tokens processed: {len(book)}\")\n",
    "print(f\"Total sentences: {len(list(book.sents))}\")\n",
    "print(f\"Total entities found: {len(book.ents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db65ae61-bdd6-48bc-87af-21b381ea67e6",
   "metadata": {},
   "source": [
    "## 6. Visualize Sample of Identified Entities\n",
    "\n",
    "Next I'll examine a sample of what the NER algorithm detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86352ec8-b752-446f-a05c-b26ee1e42917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of identified entities (first 50):\n",
      "\n",
      "the 20th century               | DATE            | DATE\n",
      "Search            Search                       Appearance                 Donate  Create | WORK_OF_ART     | WORK_OF_ART\n",
      "Log in         Personal        | ORG             | ORG\n",
      "1                              | CARDINAL        | CARDINAL\n",
      "1900s                          | DATE            | DATE\n",
      "1.1                            | CARDINAL        | CARDINAL\n",
      "1.3                            | CARDINAL        | CARDINAL\n",
      "1.4                            | CARDINAL        | CARDINAL\n",
      "1910s                          | DATE            | DATE\n",
      "2.1                            | CARDINAL        | CARDINAL\n",
      "2.3 1912         2.4 1913         2.5 1914         2.6 1915         2.7 1916         2.8 1917         2.9 1918 | PRODUCT         | PRODUCT\n",
      "3.6                            | CARDINAL        | CARDINAL\n",
      "3.9                            | CARDINAL        | CARDINAL\n",
      "3.10                           | CARDINAL        | CARDINAL\n",
      "1930s                          | DATE            | DATE\n",
      "1930s                          | DATE            | DATE\n",
      "4.1                            | CARDINAL        | CARDINAL\n",
      "1930                           | DATE            | DATE\n",
      "4.4                            | CARDINAL        | CARDINAL\n",
      "4.7                            | CARDINAL        | CARDINAL\n",
      "4.8                            | CARDINAL        | CARDINAL\n",
      "4.10 1939                      | DATE            | DATE\n",
      "5 1940s                        | DATE            | DATE\n",
      "1940s                          | DATE            | DATE\n",
      "5.1                            | CARDINAL        | CARDINAL\n",
      "5.4                            | CARDINAL        | CARDINAL\n",
      "5.9                            | CARDINAL        | CARDINAL\n",
      "6 1950s                        | DATE            | DATE\n",
      "1950s                          | DATE            | DATE\n",
      "6.1                            | CARDINAL        | CARDINAL\n",
      "1950                           | DATE            | DATE\n",
      "6.2                            | CARDINAL        | CARDINAL\n",
      "6.5                            | CARDINAL        | CARDINAL\n",
      "6.7                            | CARDINAL        | CARDINAL\n",
      "6.9                            | CARDINAL        | CARDINAL\n",
      "6.10                           | CARDINAL        | CARDINAL\n",
      "1960s                          | DATE            | DATE\n",
      "1960s                          | DATE            | DATE\n",
      "7.1                            | CARDINAL        | CARDINAL\n",
      "7.3                            | CARDINAL        | CARDINAL\n",
      "7.4                            | CARDINAL        | CARDINAL\n",
      "1963                           | DATE            | DATE\n",
      "7.5                            | CARDINAL        | CARDINAL\n",
      "7.8                            | CARDINAL        | CARDINAL\n",
      "7.10                           | CARDINAL        | CARDINAL\n",
      "8                              | CARDINAL        | CARDINAL\n",
      "1970s                          | DATE            | DATE\n",
      "8.1                            | CARDINAL        | CARDINAL\n",
      "1970                           | DATE            | DATE\n",
      "1971                           | DATE            | DATE\n",
      "\n",
      "\n",
      "Entity type distribution:\n",
      "DATE           : 1216\n",
      "GPE            : 850\n",
      "ORG            : 495\n",
      "PERSON         : 482\n",
      "CARDINAL       : 277\n",
      "NORP           : 232\n",
      "ORDINAL        : 164\n",
      "EVENT          : 104\n",
      "LOC            : 78\n",
      "FAC            : 42\n",
      "WORK_OF_ART    : 34\n",
      "PRODUCT        : 26\n",
      "LAW            : 18\n",
      "MONEY          : 11\n",
      "QUANTITY       : 4\n",
      "TIME           : 4\n",
      "PERCENT        : 2\n",
      "LANGUAGE       : 1\n"
     ]
    }
   ],
   "source": [
    "# Display a sample of identified entities\n",
    "print(\"Sample of identified entities (first 50):\\n\")\n",
    "for ent in list(book.ents)[:50]:\n",
    "    print(f\"{ent.text:30} | {ent.label_:15} | {ent.label_}\")\n",
    "\n",
    "print(\"\\n\\nEntity type distribution:\")\n",
    "entity_types = {}\n",
    "for ent in book.ents:\n",
    "    entity_types[ent.label_] = entity_types.get(ent.label_, 0) + 1\n",
    "\n",
    "for label, count in sorted(entity_types.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{label:15}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef7e296-809f-49fd-ac50-f16ad54f4c56",
   "metadata": {},
   "source": [
    "## 7. Split Sentence Entities\n",
    "\n",
    "I am creating a dataframe where each row contains a sentence and all the entities (GPE - Geopolitical Entities) found in that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06673df4-b9fc-4fb2-94fa-b8cc982371cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 1401\n",
      "\n",
      "Sentences with at least one GPE entity: 593\n",
      "\n",
      "First 10 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Timeline of the 20th century - Wikipedia  ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>also         14 Further reading         15 Ref...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>informationCite this pageGet shortened URLDown...</td>\n",
       "      <td>[Download]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Please help improve this article by adding cit...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unsourced material may be challenged and removed.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Find sources: \"Timeline of the 20th century\" –...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Learn how and when to remove this message)  M...</td>\n",
       "      <td>[Millennia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1900s[edit] See also: Edwardian Era, Gilded Ag...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>January 22: Edward VII became King of England ...</td>\n",
       "      <td>[Emperor, India]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>March 2: The Platt Amendment provides for Cuba...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence          entities\n",
       "0      Timeline of the 20th century - Wikipedia  ...                []\n",
       "1  also         14 Further reading         15 Ref...                []\n",
       "2  informationCite this pageGet shortened URLDown...        [Download]\n",
       "3  Please help improve this article by adding cit...                []\n",
       "4  Unsourced material may be challenged and removed.                []\n",
       "5  Find sources: \"Timeline of the 20th century\" –...                []\n",
       "6  (Learn how and when to remove this message)  M...       [Millennia]\n",
       "7  1900s[edit] See also: Edwardian Era, Gilded Ag...                []\n",
       "8  January 22: Edward VII became King of England ...  [Emperor, India]\n",
       "9  March 2: The Platt Amendment provides for Cuba...                []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of dictionaries with sentences and their entities\n",
    "df_sentences = []\n",
    "\n",
    "# Loop through sentences, extract GPE entities (countries/places)\n",
    "for sent in book.sents:\n",
    "    # Get all GPE entities from the sentence\n",
    "    entity_list = [ent.text for ent in sent.ents if ent.label_ == 'GPE']\n",
    "    df_sentences.append({\"sentence\": sent.text, \"entities\": entity_list})\n",
    "\n",
    "# Convert to dataframe\n",
    "df_sentences = pd.DataFrame(df_sentences)\n",
    "\n",
    "print(f\"Total sentences: {len(df_sentences)}\")\n",
    "print(f\"\\nSentences with at least one GPE entity: {len(df_sentences[df_sentences['entities'].map(len) > 0])}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "df_sentences.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa6ef6-197f-4457-bca2-87f885b9d276",
   "metadata": {},
   "source": [
    "## 8. Filter Entities Using Countries List\n",
    "\n",
    "I'm filtering the entities to keep only those that match countries from my scraped list. This removes non-country locations like cities or regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b3446eb-44ab-4ec5-924c-dbf0dfac5bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences with country entities: 335\n",
      "\n",
      "First 10 filtered sentences:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>entities</th>\n",
       "      <th>country_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>January 22: Edward VII became King of England ...</td>\n",
       "      <td>[Emperor, India]</td>\n",
       "      <td>[India]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>June: Emily Hobhouse reports on the poor condi...</td>\n",
       "      <td>[South Africa]</td>\n",
       "      <td>[South Africa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>September 7: The Eight-Nation Alliance defeats...</td>\n",
       "      <td>[the Boxer Rebellion, China]</td>\n",
       "      <td>[China]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>May 20: Cuba given independence by the United ...</td>\n",
       "      <td>[Cuba, the United States]</td>\n",
       "      <td>[Cuba]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Venezuelan crisis of 1902–1903, in which Brita...</td>\n",
       "      <td>[Britain, Germany, Italy, Venezuela]</td>\n",
       "      <td>[Germany, Italy, Venezuela]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>June 11: King Alexander I of Serbia and his wi...</td>\n",
       "      <td>[Serbia]</td>\n",
       "      <td>[Serbia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>In Russia the Bolsheviks and the Mensheviks fo...</td>\n",
       "      <td>[Russia, Bolsheviks]</td>\n",
       "      <td>[Russia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>November 18: Independence of Panama, the Hay–B...</td>\n",
       "      <td>[Panama, the United States, Panama]</td>\n",
       "      <td>[Panama, Panama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>April 8: Entente Cordiale signed between Brita...</td>\n",
       "      <td>[Britain, France]</td>\n",
       "      <td>[France]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1905[edit] January 22: The Revolution of 1905 ...</td>\n",
       "      <td>[Russia]</td>\n",
       "      <td>[Russia]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence  \\\n",
       "8   January 22: Edward VII became King of England ...   \n",
       "10  June: Emily Hobhouse reports on the poor condi...   \n",
       "12  September 7: The Eight-Nation Alliance defeats...   \n",
       "17  May 20: Cuba given independence by the United ...   \n",
       "21  Venezuelan crisis of 1902–1903, in which Brita...   \n",
       "23  June 11: King Alexander I of Serbia and his wi...   \n",
       "25  In Russia the Bolsheviks and the Mensheviks fo...   \n",
       "27  November 18: Independence of Panama, the Hay–B...   \n",
       "30  April 8: Entente Cordiale signed between Brita...   \n",
       "34  1905[edit] January 22: The Revolution of 1905 ...   \n",
       "\n",
       "                                entities             country_entities  \n",
       "8                       [Emperor, India]                      [India]  \n",
       "10                        [South Africa]               [South Africa]  \n",
       "12          [the Boxer Rebellion, China]                      [China]  \n",
       "17             [Cuba, the United States]                       [Cuba]  \n",
       "21  [Britain, Germany, Italy, Venezuela]  [Germany, Italy, Venezuela]  \n",
       "23                              [Serbia]                     [Serbia]  \n",
       "25                  [Russia, Bolsheviks]                     [Russia]  \n",
       "27   [Panama, the United States, Panama]             [Panama, Panama]  \n",
       "30                     [Britain, France]                     [France]  \n",
       "34                              [Russia]                     [Russia]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to filter entities - keep only countries from our list\n",
    "def filter_entity(ent_list, countries_list):\n",
    "    \"\"\"\n",
    "    Filters entity list to keep only entities that match country names\n",
    "    \"\"\"\n",
    "    return [ent for ent in ent_list if ent in countries_list]\n",
    "\n",
    "# Apply filter to create new column with only country entities\n",
    "df_sentences['country_entities'] = df_sentences['entities'].apply(\n",
    "    lambda x: filter_entity(x, countries)\n",
    ")\n",
    "\n",
    "# Filter dataframe to keep only sentences with country entities\n",
    "df_sentences_filtered = df_sentences[df_sentences['country_entities'].map(len) > 0]\n",
    "\n",
    "print(f\"Sentences with country entities: {len(df_sentences_filtered)}\")\n",
    "print(f\"\\nFirst 10 filtered sentences:\")\n",
    "df_sentences_filtered.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138f423f-a2dd-4a6c-b766-82f991285f26",
   "metadata": {},
   "source": [
    "## 9. Create Relationships Between Countries\n",
    "\n",
    "I'm analyzing which countries appear together in close proximity (within a 5-sentence window). If two countries are mentioned near each other multiple times, they likely have a historical relationship.\n",
    "\n",
    "**Method:**\n",
    "- Window size: 5 sentences\n",
    "- If two countries appear in the same window, a relationship is created\n",
    "- Relationship strength = frequency of co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f94a2d7-50df-49f6-8b4b-1be87e46d5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total relationships found: 1594\n",
      "\n",
      "First 10 relationships:\n",
      "1. India ↔ South Africa\n",
      "2. South Africa ↔ China\n",
      "3. China ↔ Cuba\n",
      "4. Cuba ↔ Germany\n",
      "5. Germany ↔ Italy\n",
      "6. Italy ↔ Venezuela\n",
      "7. South Africa ↔ China\n",
      "8. China ↔ Cuba\n",
      "9. Cuba ↔ Germany\n",
      "10. Germany ↔ Italy\n"
     ]
    }
   ],
   "source": [
    "# Create relationships based on country co-occurrences\n",
    "relationships = []\n",
    "window_size = 5\n",
    "\n",
    "# Iterate through the filtered dataframe\n",
    "for i in range(len(df_sentences_filtered)):\n",
    "    # Define end of window (max 5 sentences ahead or end of dataframe)\n",
    "    end_i = min(i + window_size, len(df_sentences_filtered))\n",
    "    \n",
    "    # Get all countries mentioned in this window\n",
    "    indices = df_sentences_filtered.index[i:end_i]\n",
    "    char_list = sum(df_sentences_filtered.loc[indices, 'country_entities'].tolist(), [])\n",
    "    \n",
    "    # Remove consecutive duplicates\n",
    "    char_unique = [char_list[j] for j in range(len(char_list))\n",
    "                   if (j == 0) or char_list[j] != char_list[j-1]]\n",
    "    \n",
    "    # Create relationships between pairs of countries\n",
    "    if len(char_unique) > 1:\n",
    "        for idx, a in enumerate(char_unique[:-1]):\n",
    "            b = char_unique[idx + 1]\n",
    "            relationships.append({\"source\": a, \"target\": b})\n",
    "\n",
    "print(f\"Total relationships found: {len(relationships)}\")\n",
    "print(f\"\\nFirst 10 relationships:\")\n",
    "for i, rel in enumerate(relationships[:10], 1):\n",
    "    print(f\"{i}. {rel['source']} ↔ {rel['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9904c7c-a8df-4e8a-8b65-aa70262d0a32",
   "metadata": {},
   "source": [
    "## 10. Aggregate & Count Relationship Frequencies\n",
    "\n",
    "I'll summarize the relationships to show how many times each country pair appears together, which will indicate the strength of their historical connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74812d6a-6bc4-4d1d-972e-4c42de21ff97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique country pairs: 326\n",
      "\n",
      "Top 20 most frequent country relationships:\n",
      "\n",
      "     source           target  value\n",
      "     Brazil            China     20\n",
      "    Austria          Germany     17\n",
      "      China            Japan     13\n",
      "Afghanistan             Iran     13\n",
      "    Germany            Japan     12\n",
      "       Iran           Poland     12\n",
      "      Italy Northern Ireland     12\n",
      "    Austria          Hungary     10\n",
      "     France          Germany     10\n",
      "  Nicaragua    United States     10\n",
      "      China        Hong Kong     10\n",
      "     France          Morocco     10\n",
      "  Indonesia         Malaysia     10\n",
      "     Israel        Palestine     10\n",
      "     France          Tunisia     10\n",
      "    Denmark           Norway     10\n",
      " Bangladesh         Pakistan     10\n",
      "     Israel          Lebanon      9\n",
      "    Germany            Italy      9\n",
      "   Portugal            Spain      9\n"
     ]
    }
   ],
   "source": [
    "# Convert to dataframe\n",
    "relationship_df = pd.DataFrame(relationships)\n",
    "\n",
    "# Sort the pairs alphabetically to ensure A->B and B->A are counted together\n",
    "relationship_df = pd.DataFrame(\n",
    "    np.sort(relationship_df.values, axis=1),\n",
    "    columns=relationship_df.columns\n",
    ")\n",
    "\n",
    "# Add value column for counting\n",
    "relationship_df[\"value\"] = 1\n",
    "\n",
    "# Group by source-target pairs and sum frequencies\n",
    "relationship_df = relationship_df.groupby(\n",
    "    [\"source\", \"target\"], \n",
    "    sort=False, \n",
    "    as_index=False\n",
    ").sum()\n",
    "\n",
    "# Sort by frequency (highest first)\n",
    "relationship_df = relationship_df.sort_values('value', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"Unique country pairs: {len(relationship_df)}\")\n",
    "print(f\"\\nTop 20 most frequent country relationships:\\n\")\n",
    "print(relationship_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46656e-bd2d-4697-bb7c-76d52013bd44",
   "metadata": {},
   "source": [
    "## 11. Export Relationships Dataframe\n",
    "\n",
    "I'm saving the relationships dataframe as a CSV file for use in later exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2573ac69-f2bf-4af3-aca0-0ace8cdecf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationships dataframe exported successfully!\n",
      "File: country_relationships.csv\n",
      "Total rows: 326\n",
      "\n",
      "Dataframe summary:\n",
      "- Unique countries involved: 105\n",
      "- Strongest relationship: Brazil ↔ China (20 co-occurrences)\n",
      "- Average relationship strength: 4.89\n"
     ]
    }
   ],
   "source": [
    "# Export relationships dataframe to CSV\n",
    "relationship_df.to_csv('country_relationships.csv', index=False)\n",
    "\n",
    "print(\"Relationships dataframe exported successfully!\")\n",
    "print(f\"File: country_relationships.csv\")\n",
    "print(f\"Total rows: {len(relationship_df)}\")\n",
    "print(f\"\\nDataframe summary:\")\n",
    "print(f\"- Unique countries involved: {len(set(relationship_df['source'].tolist() + relationship_df['target'].tolist()))}\")\n",
    "print(f\"- Strongest relationship: {relationship_df.iloc[0]['source']} ↔ {relationship_df.iloc[0]['target']} ({relationship_df.iloc[0]['value']} co-occurrences)\")\n",
    "print(f\"- Average relationship strength: {relationship_df['value'].mean():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "20th_century",
   "language": "python",
   "name": "20th_century"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
